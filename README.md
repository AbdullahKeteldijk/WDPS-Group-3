# wdps2017
Web Data Processing Systems 2017 (VU course XM_40020)

# Assignment 1: Large Scale Entity Linking
The first assignment for this course is to perform [Entity Linking](https://en.wikipedia.org/wiki/Entity_linking) on a collection of web pages. Your solution should be scalable and accurate, and conform to the specifications below. You should work in groups of 3 or 4 people, and use a version control system that we can access. After forming a group, set up a private version control repository using [BitBucket](https://bitbucket.org) or [Github](http://github.com) and give me access (my username is bennokr on both websites). If you need help with this, contact me. You can use any existing languages or tools you want, as long as it's easy for us to run it on the DAS-4 cluster. Of course, your solution is not allowed to call web services over the internet. You are encouraged to use the technologies covered in the lectures.

Your solution should be runnable trough a bash script with the command line options specified below. An example of some dummy starter code is available in this repository.

The input data is a gzipped [WARC file](https://en.wikipedia.org/wiki/Web_ARChive), and the ouput is a three-column tab-separated file with document IDs, entity surface forms (like "Berlin"), and Freebase entity IDs (like "/m/03hrz"). There is a sample file of the input (warc) and output (tsv) formats in this directory. Your program must accept two command line arguments: the WARC key name that is used in the web archive for identifying documents (like "WARC-Record-ID"), the HDFS path of the input and the HDFS path of the output. Your program must be runnable on the DAS4 cluster using a bash script, and you should provide a README file with a description of your approach. For example, your program could be run using the command `bash run.sh "WARC-Record-ID" "hdfs://user/yourname/input.warc.gz" > output.tsv`.

You will be graded based on whether your solution conforms to the assignment, on its scalability and on its [F1 score](https://en.wikipedia.org/wiki/F1_score). A script to calculate this score using gold standard data and the prediction data is in the same folder as the format sample data. An input file for you to work with (from [CommonCrawl](http://commoncrawl.org)) is on the Hadoop file system of DAS-4, at hdfs:///user/bbkruit/CC-MAIN-20160924173739-00000-ip-10-143-35-109.ec2.internal.warc.gz . You can view it using hdfs `dfs -cat hdfs:///user/bbkruit/CC-MAIN-20160924173739-00000-ip-10-143-35-109.ec2.internal.warc.gz | zcat | less` (see also https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html). We will be running your program on a different web archive dataset with a different WARC key name (something other than "WARC-Record-ID").

We have set up two REST services for you to use on the DAS-4 cluster. One is an [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/index.html) instance that contains labels for Freebase IDs. It can be accessed from the command line like this: `curl "http://10.149.0.127:9200/freebase/label/_search?q=obama"` . The other is a SPARQL endpoint that can be accessed like this: `curl -s -XPOST "http://localhost:5210/sparql" -d "print=true&query=select+distinct+%3FConcept+where+%7B%5B%5D+a+%3FConcept%7D+LIMIT+100"`. To experiment with some sparql examples, see https://query.wikidata.org/ . This one is not yet available on DAS-4, but we will let you know when it is. Both services return JSON. Because Freebase was integrated into the Google Knowledge Graph, you can look up IDs on Google using URLs like this: [http://g.co/kg/m/03hrz].
